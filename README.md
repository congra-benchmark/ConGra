

![logo](./assets/logo.png)


--------------------------------------------------------------------------------
This is the official repository for the ConGra dataset and benchmark. In this repository, we provide a large-scale multilingual conflict resolution dataset, as well as the benchmark reproduce steps.

## ConGra Dataset
### Download Datasets 
Our ConGra dataset is available at [ConGra Dataset](https://figshare.com/ndownloader/files/46967428).

You can download this dataset by the following script.
```sh
wget -c -O ConGra_dataset.tar.gz https://figshare.com/ndownloader/files/46967428
```
Check the md5sum for data integrity. 
```
md5sum ConGra_dataset.tar.gz
869a312f577adcfe3a8654314a56d2a3  ConGra_dataset.tar.gz
```
Please extract the dataset into the ```data``` folder.
```
tar -xzvf ConGra_dataset.tar.gz -C data
```

### File Structure of Dataset

```data
raw_datasets
└── <language_dir>
          └──<project_dir>
                    └──<conflict_pair_dir>
                                ├──a
                                ├──b
                                ├──base
                                ├──merged
                                ├──merged_without_base
                                ├──regions
                                └──resolved

congra_{full,tiny}_datasets
└── <language_dir>
          └──<classified_dir>
                    ├──<conflict_file>
                    └──meta_list.txt
```

#### raw_datasets

**raw_datasets** contain raw conflict data and corresponding information that may used to evaluate merging tools. 

##### <language_dir>
Specify files written in various programming languages. Under raw_datasets, `C_C++`, `Java`, and `Python` are supported.

##### <project_dir>
Specify files belonging to various projects.

##### <conflict_pair_dir>
Indicate files that differ in two given commit indexes.
Under this directory, seven sub-directories are created:

- **a**: Conflict files in A.
- **b**: Conflict files in B.
- **base**: Conflict files in version O, where Base is the nearest common ancestor of A and B on the commit graph.
- **merged**: Conflict files in version M in diff3 format.
- **merged_without_base**: Conflict files in version M in diff3 format, but ignoring the code from O.
- **resolved**: Resolved conflict files in version R.
- **regions**: The resolved conflict regions (shown in line number scopes) in the corresponding files of R.


#### congra_{full,tiny}_datasets

**congra_{full,tiny}_datasets** are constructed to provide a classified, complexity-graded dataset based on **raw_datasets** for ConGra's evaluation.

We built two types of ConGra dataset. **congra_tiny_datasets** ensures a one-to-one mapping between file and conflict, i.e., each file contains only one conflict, while **congra_full_datasets** may include files with multiple conflicts.
**congra_tiny_datasets** is a subset of **congra_full_datasets**.

##### <language_dir>
Specify files written in various programming languages. Under congra_{full,tiny}_datasets, `c`, `cpp`, `java`, and `python` are supported.

##### <classified_dir>
Specify the classifications of conflict files. `text`, `sytx`, `func`, `text+sytx`, `text+func`, `sytx+func`, and `text+sytx+func` are supported.

##### <conflict_file>
Source file with conflicts. The name of this file has been hashed with salt.

##### meta_list.txt
The conflict conclusion of the current classification. This file contains multiple lines, which each line looks like:

```
<location_in_raw_datasets>: <hashed_name>: <conflict_num>
```
<location_in_raw_datasets> indicates the relative path of the conflict file under **raw_datasets**. <hashed_name> indicates the hashed name of the conflict file. <conflict_num> specifies the <conflict_num>th conflict in the file <hashed_name> belongs to this category.

## Reproduce Benchmark Results 
### 0. Experimental Enviroments
Our code works with the python 3.9, pytorch, cudatoolkit and cudnn. In addition, our code requires the following dependency packages:
```
transformers==4.39.0.dev0
langchain==0.1.15
langchain-anthropic==0.1.11
langchain-aws==0.1.3
langchain-community==0.0.38
langchain-core==0.1.52
langchain-google-genai==1.0.3
langchain-groq==0.1.3
langchain-openai==0.1.6
langchain-text-splitters==0.0.2
langsmith==0.1.59
vllm==0.3.3
```
You can use higher versions of the dependency libraries to achieve better performance.
### 1. Download datasets
Please download the dataset and extract it to the ```data``` directory, following the file structure below.
```
├── data
│   ├── congra_full_datasets
│   ├── congra_tiny_datasets
│   ├── raw_datasets
│   └── README.md
├── README.md
├── src
```
### 2. Run the model to obtain conflict resolutions
Please refer to the following command to obtain conflict resolution solutions generated by the model. By default, we are running the **ConGRA-full** dataset.
```sh
python main.py --llm_model {model_name} --temperature 0.7 --api_key {your_apikey} --language {python/jave/c/cpp} --worker {1/2/3} --type {'text','text_func','text_sytx','text_sytx_func','func','sytx','sytx_func'}
```
For example, We use ```deepseek-coder``` to obtain conflict resolutions. Please refer to [the DeepSeek Open Platform](https://platform.deepseek.com/api-docs/) to obtain the API_KEY first.
```sh
cd src 
export deepseek_key="sk-xxxx" # Please replace with your API_KEY
python main.py --llm_model deepseek-coder --temperature 0.7 --api_key $deepseek_key --language python --worker 1 --type func
```
The results are saved at ```output/python/func/deepseek-coder_0.7```, where ```resolutions.jsonl``` summarizes the experimental process and final results. We have provided an [example](output/python/func/deepseek-coder_0.7/resolutions.jsonl) output for reference.

If you want to use a local LLM, we recommend deploying the LLM using [vllm](https://github.com/vllm-project/vllm). Please refer to the following example command to run the specified local model.
```sh
model='meta-llama/CodeLlama-7b-Instruct-hf'
gpu='0,1,2,3,4,5,6,7' 
port=1234
CUDA_VISIBLE_DEVICES="$gpu" python -m vllm.entrypoints.openai.api_server --model "$model" --port "$port" --tensor-parallel-size 8 --gpu-memory-utilization 0.97
```
